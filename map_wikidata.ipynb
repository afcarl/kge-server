{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def get_bne_wikidata(max_number=100):\n",
    "    key = \"bne\"\n",
    "    value = \"wikidata\"\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "    SELECT ?bne ?wikidata\n",
    "    WHERE { \n",
    "        ?wikidata wdt:P950 ?bne .\n",
    "    }\n",
    "    LIMIT \"\"\"+str(max_number)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return [(item[key]['value'], item[value]['value']) for item in json_data]\n",
    "\n",
    "\n",
    "datos = get_bne_wikidata()\n",
    "#for key,value in datos:\n",
    "    #print(key,\"\\t\",value)\n",
    "\n",
    "f = open(\"wikidatamap.bin\", \"wb+\")\n",
    "pickle.dump(datos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "query2 = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct { ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }\n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "    ?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "} LIMIT 10000 OFFSET 1880000\n",
    "\"\"\"\n",
    "datos.load_dataset_from_query(query2, only_uri=False)\n",
    "\n",
    "\n",
    "#datos.load_dataset_from_nlevels(5, extra_params=\"LIMIT 100\")\n",
    "datos.show(verbose=False)\n",
    "dataset1 = datos.train_split()\n",
    "\n",
    "#     print(sub,dataset1[sub])\n",
    "datos.save_to_binary(\"wikidata_2.bin\")\n",
    "\n",
    "# datos.subs\n",
    "\n",
    "#print(len(jsonlist))\n",
    "#jsonlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rel = [i[2] for i in datos.subs]\n",
    "print(len(datos.relations_dict))\n",
    "print(len(datos.relations))\n",
    "print(len(datos.entities_dict))\n",
    "print(len(datos.entities))\n",
    "rel = np.array(rel)\n",
    "print(np.sort(rel))\n",
    "data = np.matrix(datos.subs)\n",
    "print(data[:,2])\n",
    "\n",
    "print('Splitting the data into training and validation sets ...')\n",
    "print(data.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:-int(nb_validation_samples/2)]\n",
    "x_test = data[-int(nb_validation_samples/2):]\n",
    "\n",
    "print(x_val.shape,\"\\n\\n\\n\",x_test.shape,\"\\n\\n\\n\",x_train.shape)\n",
    "print(x_test.shape[0]+x_train.shape[0]+x_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14951 entities, 1345 relations, 592213 tripletas\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import kgeserver.dataset as dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "fb_test = open(\"FB15k/freebase_mtr100_mte100-test.txt\", \"r\")\n",
    "fb_train = open(\"FB15k/freebase_mtr100_mte100-train.txt\", \"r\")\n",
    "fb_valid = open(\"FB15k/freebase_mtr100_mte100-valid.txt\", \"r\")\n",
    "datos.load_dataset_from_csv(fb_train, \"\\t\")\n",
    "datos.load_dataset_from_csv(fb_test, \"\\t\")\n",
    "datos.load_dataset_from_csv(fb_valid, \"\\t\")\n",
    "\n",
    "datos.show(verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_json_offset(n, count):\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct {{ ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }}\n",
    "WHERE {{ ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "}} LIMIT {0} OFFSET {1}\n",
    "\"\"\".format(count, n*count)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    if response.status_code is not 200:\n",
    "        return False, \"Error occurred on http request. Code\"+str(response.status_code)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return json_data\n",
    "\n",
    "all_json = []\n",
    "for query in range(0,30):\n",
    "    json1 = get_json_offset(query,20000)\n",
    "    all_json = all_json + json1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning level 1 with 2000 elements\n",
      "Enter S to show status: s\n",
      "Elapsed time: 1s. Depth 1 of 3. Entities scanned: 4.35% (87 of 2000) Active threads: 36\n",
      "Waiting all threads to end\n",
      "Scanning level 2 with 2000 elements\n",
      "Waiting all threads to end\n",
      "Scanning level 3 with 2000 elements\n",
      "Waiting all threads to end\n",
      "38614 entities, 382 relations, 93288 tripletas\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import kgeserver.dataset as dataset\n",
    "import kgeserver.wikidata_dataset as wikidata_dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(wikidata_dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dtset = wikidata_dataset.WikidataDataset()\n",
    "\n",
    "\n",
    "#dataset.load_entire_dataset(1)\n",
    "dtset.load_dataset_recurrently(3, verbose=2, limit_ent=2000)\n",
    "dtset.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dtset.relations[8])\n",
    "dtset.save_to_binary(\"wikidata_25k.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import algorithm\n",
    "import skge\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dt = dataset.Dataset()\n",
    "\n",
    "dt.load_from_binary(\"wn18.bin\")\n",
    "\n",
    "al = algorithm.Algorithm(dt)\n",
    "\n",
    "models = models = al.find_best(ncomps=[50], margins=[2], model_types=[skge.TransE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import algorithm\n",
    "import skge\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dt = dataset.Dataset()\n",
    "\n",
    "dt.load_from_binary(\"wn18.bin\")\n",
    "\n",
    "al = algorithm.Algorithm(dt)\n",
    "\n",
    "models = models = al.find_best(ncomps=[150], margins=[0.2], model_types=[skge.HolE])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
