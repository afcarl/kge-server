{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def get_bne_wikidata(max_number=100):\n",
    "    key = \"bne\"\n",
    "    value = \"wikidata\"\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "    SELECT ?bne ?wikidata\n",
    "    WHERE { \n",
    "        ?wikidata wdt:P950 ?bne .\n",
    "    }\n",
    "    LIMIT \"\"\"+str(max_number)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return [(item[key]['value'], item[value]['value']) for item in json_data]\n",
    "\n",
    "\n",
    "datos = get_bne_wikidata()\n",
    "#for key,value in datos:\n",
    "    #print(key,\"\\t\",value)\n",
    "\n",
    "f = open(\"wikidatamap.bin\", \"wb+\")\n",
    "pickle.dump(datos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error on endpoint. HTTP status code: 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-42d771d9da45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m } LIMIT 10000 OFFSET 1880000\n\u001b[1;32m     15\u001b[0m \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdatos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_from_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/work/dataset.py\u001b[0m in \u001b[0;36mload_dataset_from_query\u001b[0;34m(self, query, only_uri)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWIKIDATA_ENDPOINT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error on endpoint. HTTP status code: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mjsonlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bindings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error on endpoint. HTTP status code: 500"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "query2 = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct { ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }\n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "    ?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "} LIMIT 10000 OFFSET 1880000\n",
    "\"\"\"\n",
    "datos.load_dataset_from_query(query2, only_uri=False)\n",
    "\n",
    "\n",
    "#datos.load_dataset_from_nlevels(5, extra_params=\"LIMIT 100\")\n",
    "datos.show(verbose=False)\n",
    "dataset1 = datos.train_split()\n",
    "\n",
    "#     print(sub,dataset1[sub])\n",
    "datos.save_to_binary(\"wikidata_2.bin\")\n",
    "\n",
    "# datos.subs\n",
    "\n",
    "#print(len(jsonlist))\n",
    "#jsonlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "440\n",
      "1497\n",
      "1497\n",
      "[  0   0   0 ..., 438 438 439]\n",
      "[[  0]\n",
      " [  1]\n",
      " [  3]\n",
      " ..., \n",
      " [220]\n",
      " [439]\n",
      " [ 31]]\n",
      "Splitting the data into training and validation sets ...\n",
      "(1727, 3)\n",
      "(173, 3) \n",
      "\n",
      "\n",
      " (172, 3) \n",
      "\n",
      "\n",
      " (1382, 3)\n",
      "1727\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rel = [i[2] for i in datos.subs]\n",
    "print(len(datos.relations_dict))\n",
    "print(len(datos.relations))\n",
    "print(len(datos.entities_dict))\n",
    "print(len(datos.entities))\n",
    "rel = np.array(rel)\n",
    "print(np.sort(rel))\n",
    "data = np.matrix(datos.subs)\n",
    "print(data[:,2])\n",
    "\n",
    "print('Splitting the data into training and validation sets ...')\n",
    "print(data.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:-int(nb_validation_samples/2)]\n",
    "x_test = data[-int(nb_validation_samples/2):]\n",
    "\n",
    "print(x_val.shape,\"\\n\\n\\n\",x_test.shape,\"\\n\\n\\n\",x_train.shape)\n",
    "print(x_test.shape[0]+x_train.shape[0]+x_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "datos.load_from_binary(\"wdata2lvl.bin\")\n",
    "datos.show(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_json_offset(n, count):\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct {{ ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }}\n",
    "WHERE {{ ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "}} LIMIT {0} OFFSET {1}\n",
    "\"\"\".format(count, n*count)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    if response.status_code is not 200:\n",
    "        return False, \"Error occurred on http request. Code\"+str(response.status_code)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return json_data\n",
    "\n",
    "all_json = []\n",
    "for query in range(0,30):\n",
    "    json1 = get_json_offset(query,20000)\n",
    "    all_json = all_json + json1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R0: Offset: 0, limit: 2500\n",
      "R1: Offset: 2500, limit: 2500\n",
      "R2: Offset: 5000, limit: 2500\n",
      "R3: Offset: 7500, limit: 2500\n",
      "finished. Saving\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import queries\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(queries)\n",
    "\n",
    "query = queries.Queries()\n",
    "\n",
    "js = query.big_query(n_levels=3, n_rounds=4, total=10000)\n",
    "print(\"finished. Saving\")\n",
    "f1 = open(\"millon.json\", \"wb+\")\n",
    "pickle.dump(js, f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32563 entities\n",
      "Scanning level 1 with 32701 elements\n",
      "Waiting all threads to end\n",
      "Scanning level 2 with 11560 elements\n",
      "Waiting all threads to end\n",
      "Scanning level 3 with 1577 elements\n",
      "Waiting all threads to end\n",
      "13767 entities, 730 relations, 15079 tripletas\n",
      "Enter S to show status: q\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dtset = dataset.Dataset()\n",
    "\n",
    "\n",
    "#dataset.load_entire_dataset(1)\n",
    "dtset.load_dataset_recurrently(3, verbose=2)\n",
    "dtset.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13767 entities, 730 relations, 15079 tripletas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtset.show()\n",
    "dtset.save_to_binary(\"wdata_15k.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8363 entities, 930 relations, 9996 tripletas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dataset = dataset.Dataset()\n",
    "\n",
    "\n",
    "#dataset.load_entire_dataset(1)\n",
    "dataset.load_from_binary(\"wikidata_1.bin\")\n",
    "dataset.show()\n",
    "dataset.save_to_binary(\"wikidata_10k.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times= [10, 60, 120, 180, 300, 420, 500, 600, 700, 800, 900]\n",
    "try_catch_all = [548, 2635, None, 7216, 12142, 15622, None, 20500, 23986, 28086, 31591]\n",
    "if_else_all=[550, 3369, 6771, 10057, 16666, 23575, 28325, None, None, None]\n",
    "if_else_all2 = [818,3654,6924,17093,None,28298]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0014716405210133468, 40)\n",
      "(0.0012535442024012538, 170)\n",
      "(0.0012300529957259387, 30)\n",
      "(0.0012139665638769258, 20)\n",
      "(0.0011724069840263623, 180)\n",
      "(0.0011681899014313088, 150)\n",
      "(0.0011434153470357338, 50)\n",
      "(0.0011307594521970806, 200)\n",
      "(0.0011307594521970806, 200)\n",
      "(0.0009720014395082548, 160)\n",
      "(0.0009550920787615909, 140)\n",
      "(0.0009353911486633044, 120)\n",
      "(0.0009266319765554006, 10)\n",
      "(0.000901344774094369, 60)\n",
      "(0.000893710007412643, 190)\n",
      "(0.0008380033983167916, 110)\n",
      "(0.0008357209804546799, 70)\n",
      "(0.0008237571757411388, 130)\n",
      "(0.0007649375949932464, 80)\n",
      "(0.0007607878178162447, 100)\n",
      "(0.000691114796483639, 90)\n"
     ]
    }
   ],
   "source": [
    "scores = [{'epoch': 10, 'score': 0.00092663197655540059, 'type': 'FMRR'}, {'epoch': 20, 'score': 0.0012139665638769258, 'type': 'FMRR'}, {'epoch': 30, 'score': 0.0012300529957259387, 'type': 'FMRR'}, {'epoch': 40, 'score': 0.0014716405210133468, 'type': 'FMRR'}, {'epoch': 50, 'score': 0.0011434153470357338, 'type': 'FMRR'}, {'epoch': 60, 'score': 0.000901344774094369, 'type': 'FMRR'}, {'epoch': 70, 'score': 0.00083572098045467987, 'type': 'FMRR'}, {'epoch': 80, 'score': 0.00076493759499324644, 'type': 'FMRR'}, {'epoch': 90, 'score': 0.00069111479648363897, 'type': 'FMRR'}, {'epoch': 100, 'score': 0.0007607878178162447, 'type': 'FMRR'}, {'epoch': 110, 'score': 0.00083800339831679158, 'type': 'FMRR'}, {'epoch': 120, 'score': 0.00093539114866330437, 'type': 'FMRR'}, {'epoch': 130, 'score': 0.00082375717574113881, 'type': 'FMRR'}, {'epoch': 140, 'score': 0.00095509207876159094, 'type': 'FMRR'}, {'epoch': 150, 'score': 0.0011681899014313088, 'type': 'FMRR'}, {'epoch': 160, 'score': 0.00097200143950825478, 'type': 'FMRR'}, {'epoch': 170, 'score': 0.0012535442024012538, 'type': 'FMRR'}, {'epoch': 180, 'score': 0.0011724069840263623, 'type': 'FMRR'}, {'epoch': 190, 'score': 0.00089371000741264302, 'type': 'FMRR'}, {'epoch': 200, 'score': 0.0011307594521970806, 'type': 'FMRR'}, {'epoch': 200, 'score': 0.0011307594521970806, 'type': 'FMRR'}]\n",
    "tuples = [(e['score'], e['epoch']) for e in scores]\n",
    "for t in sorted(tuples, key=lambda t: t[0], reverse=True) :\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "times= [0.2, 2], [50,60,70,80,90,100,110,120,130,140,150],[True,False]\n",
    "todo = list(itertools.product(*times))\n",
    "print (len(todo))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
