{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def get_bne_wikidata(max_number=100):\n",
    "    key = \"bne\"\n",
    "    value = \"wikidata\"\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "    SELECT ?bne ?wikidata\n",
    "    WHERE { \n",
    "        ?wikidata wdt:P950 ?bne .\n",
    "    }\n",
    "    LIMIT \"\"\"+str(max_number)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return [(item[key]['value'], item[value]['value']) for item in json_data]\n",
    "\n",
    "\n",
    "datos = get_bne_wikidata()\n",
    "#for key,value in datos:\n",
    "    #print(key,\"\\t\",value)\n",
    "\n",
    "f = open(\"wikidatamap.bin\", \"wb+\")\n",
    "pickle.dump(datos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Error on endpoint. HTTP status code: 500",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-42d771d9da45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m } LIMIT 10000 OFFSET 1880000\n\u001b[1;32m     15\u001b[0m \"\"\"\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdatos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset_from_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_uri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/work/dataset.py\u001b[0m in \u001b[0;36mload_dataset_from_query\u001b[0;34m(self, query, only_uri)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWIKIDATA_ENDPOINT\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error on endpoint. HTTP status code: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mjsonlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"results\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"bindings\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Error on endpoint. HTTP status code: 500"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "query2 = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct { ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }\n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "    ?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "} LIMIT 10000 OFFSET 1880000\n",
    "\"\"\"\n",
    "datos.load_dataset_from_query(query2, only_uri=False)\n",
    "\n",
    "\n",
    "#datos.load_dataset_from_nlevels(5, extra_params=\"LIMIT 100\")\n",
    "datos.show(verbose=False)\n",
    "dataset1 = datos.train_split()\n",
    "\n",
    "#     print(sub,dataset1[sub])\n",
    "datos.save_to_binary(\"wikidata_2.bin\")\n",
    "\n",
    "# datos.subs\n",
    "\n",
    "#print(len(jsonlist))\n",
    "#jsonlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n",
      "440\n",
      "1497\n",
      "1497\n",
      "[  0   0   0 ..., 438 438 439]\n",
      "[[  0]\n",
      " [  1]\n",
      " [  3]\n",
      " ..., \n",
      " [220]\n",
      " [439]\n",
      " [ 31]]\n",
      "Splitting the data into training and validation sets ...\n",
      "(1727, 3)\n",
      "(173, 3) \n",
      "\n",
      "\n",
      " (172, 3) \n",
      "\n",
      "\n",
      " (1382, 3)\n",
      "1727\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rel = [i[2] for i in datos.subs]\n",
    "print(len(datos.relations_dict))\n",
    "print(len(datos.relations))\n",
    "print(len(datos.entities_dict))\n",
    "print(len(datos.entities))\n",
    "rel = np.array(rel)\n",
    "print(np.sort(rel))\n",
    "data = np.matrix(datos.subs)\n",
    "print(data[:,2])\n",
    "\n",
    "print('Splitting the data into training and validation sets ...')\n",
    "print(data.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:-int(nb_validation_samples/2)]\n",
    "x_test = data[-int(nb_validation_samples/2):]\n",
    "\n",
    "print(x_val.shape,\"\\n\\n\\n\",x_test.shape,\"\\n\\n\\n\",x_train.shape)\n",
    "print(x_test.shape[0]+x_train.shape[0]+x_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40943 entities, 18 relations, 151442 tripletas\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "datos.load_from_binary(\"wn18.bin\")\n",
    "datos.show(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_json_offset(n, count):\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct {{ ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }}\n",
    "WHERE {{ ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "}} LIMIT {0} OFFSET {1}\n",
    "\"\"\".format(count, n*count)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    if response.status_code is not 200:\n",
    "        return False, \"Error occurred on http request. Code\"+str(response.status_code)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return json_data\n",
    "\n",
    "all_json = []\n",
    "for query in range(0,30):\n",
    "    json1 = get_json_offset(query,20000)\n",
    "    all_json = all_json + json1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R0: Offset: 0, limit: 2500\n",
      "R1: Offset: 2500, limit: 2500\n",
      "R2: Offset: 5000, limit: 2500\n",
      "R3: Offset: 7500, limit: 2500\n",
      "finished. Saving\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import queries\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(queries)\n",
    "\n",
    "query = queries.Queries()\n",
    "\n",
    "js = query.big_query(n_levels=3, n_rounds=4, total=10000)\n",
    "print(\"finished. Saving\")\n",
    "f1 = open(\"millon.json\", \"wb+\")\n",
    "pickle.dump(js, f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32527 entities\n",
      "Scanning level 1 with 32663 elements\n",
      "Enter S to show status: s\n",
      "Elapsed time: 2s. Depth 1 of 1. Entities scanned: 0.56% (183 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 6s. Depth 1 of 1. Entities scanned: 1.59% (519 of 32663) Active threads: 35\n",
      "Enter S to show status: s\n",
      "Elapsed time: 9s. Depth 1 of 1. Entities scanned: 2.34% (764 of 32663) Active threads: 37\n",
      "Enter S to show status: s\n",
      "Elapsed time: 10s. Depth 1 of 1. Entities scanned: 2.50% (818 of 32663) Active threads: 35\n",
      "Enter S to show status: s\n",
      "Elapsed time: 31s. Depth 1 of 1. Entities scanned: 6.18% (2017 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 42s. Depth 1 of 1. Entities scanned: 8.06% (2633 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 47s. Depth 1 of 1. Entities scanned: 8.93% (2916 of 32663) Active threads: 30\n",
      "Enter S to show status: s\n",
      "Elapsed time: 56s. Depth 1 of 1. Entities scanned: 10.39% (3393 of 32663) Active threads: 27\n",
      "Enter S to show status: s\n",
      "Elapsed time: 59s. Depth 1 of 1. Entities scanned: 10.89% (3556 of 32663) Active threads: 35\n",
      "Enter S to show status: s\n",
      "Elapsed time: 59s. Depth 1 of 1. Entities scanned: 11.00% (3594 of 32663) Active threads: 33\n",
      "Enter S to show status: s\n",
      "Elapsed time: 60s. Depth 1 of 1. Entities scanned: 11.19% (3654 of 32663) Active threads: 22\n",
      "Enter S to show status: s\n",
      "Elapsed time: 89s. Depth 1 of 1. Entities scanned: 16.04% (5240 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 117s. Depth 1 of 1. Entities scanned: 20.69% (6759 of 32663) Active threads: 37\n",
      "Enter S to show status: s\n",
      "Elapsed time: 120s. Depth 1 of 1. Entities scanned: 21.20% (6924 of 32663) Active threads: 35\n",
      "Enter S to show status: s\n",
      "Elapsed time: 241s. Depth 1 of 1. Entities scanned: 41.89% (13684 of 32663) Active threads: 35\n",
      "Enter S to show status: s\n",
      "Elapsed time: 281s. Depth 1 of 1. Entities scanned: 48.92% (15980 of 32663) Active threads: 34\n",
      "Enter S to show status: s\n",
      "Elapsed time: 291s. Depth 1 of 1. Entities scanned: 50.71% (16563 of 32663) Active threads: 29\n",
      "Enter S to show status: s\n",
      "Elapsed time: 297s. Depth 1 of 1. Entities scanned: 51.78% (16913 of 32663) Active threads: 28\n",
      "Enter S to show status: s\n",
      "Elapsed time: 299s. Depth 1 of 1. Entities scanned: 52.14% (17030 of 32663) Active threads: 37\n",
      "Enter S to show status: s\n",
      "Elapsed time: 300s. Depth 1 of 1. Entities scanned: 52.33% (17093 of 32663) Active threads: 30\n",
      "Enter S to show status: s\n",
      "Elapsed time: 347s. Depth 1 of 1. Entities scanned: 60.43% (19739 of 32663) Active threads: 30\n",
      "Enter S to show status: s\n",
      "Elapsed time: 364s. Depth 1 of 1. Entities scanned: 63.33% (20687 of 32663) Active threads: 31\n",
      "Enter S to show status: s\n",
      "Elapsed time: 392s. Depth 1 of 1. Entities scanned: 68.17% (22268 of 32663) Active threads: 33\n",
      "Enter S to show status: s\n",
      "Elapsed time: 438s. Depth 1 of 1. Entities scanned: 76.26% (24910 of 32663) Active threads: 26\n",
      "Enter S to show status: s\n",
      "Elapsed time: 464s. Depth 1 of 1. Entities scanned: 80.64% (26341 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 483s. Depth 1 of 1. Entities scanned: 83.69% (27335 of 32663) Active threads: 37\n",
      "Enter S to show status: s\n",
      "Elapsed time: 495s. Depth 1 of 1. Entities scanned: 85.80% (28025 of 32663) Active threads: 26\n",
      "Enter S to show status: s\n",
      "Elapsed time: 498s. Depth 1 of 1. Entities scanned: 86.27% (28180 of 32663) Active threads: 37\n",
      "Enter S to show status: s\n",
      "Elapsed time: 500s. Depth 1 of 1. Entities scanned: 86.64% (28298 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 547s. Depth 1 of 1. Entities scanned: 95.04% (31044 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 552s. Depth 1 of 1. Entities scanned: 95.91% (31327 of 32663) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 567s. Depth 1 of 1. Entities scanned: 98.53% (32184 of 32663) Active threads: 23\n",
      "Enter S to show status: s\n",
      "Elapsed time: 571s. Depth 1 of 1. Entities scanned: 99.34% (32447 of 32663) Active threads: 28\n",
      "Waiting all threads to end\n",
      "1116818 entities, 1500 relations, 1441186 tripletas\n",
      "Enter S to show status: q\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dtset = dataset.Dataset()\n",
    "\n",
    "\n",
    "#dataset.load_entire_dataset(1)\n",
    "dtset.load_dataset_recurrently(1, verbose=2)\n",
    "dtset.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Found 32472 entities\n",
    "Scanning level 1 with 32606 elements\n",
    "\n",
    "Exception in thread Thread-24708:\n",
    "Traceback (most recent call last):\n",
    "  File \"/opt/conda/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
    "    self.run()\n",
    "  File \"/opt/conda/lib/python3.5/threading.py\", line 862, in run\n",
    "    self._target(*self._args, **self._kwargs)\n",
    "  File \"/home/jovyan/work/dataset.py\", line 261, in __all_entity_triplet__\n",
    "    id_obj = self.add_element(element, self.entities, self.entities_dict)\n",
    "  File \"/home/jovyan/work/dataset.py\", line 109, in extract_entity\n",
    "    if uri[2] == 'www.wikidata.org' and (uri[3] == \"reference\" and filters['wdt-reference']):\n",
    "IndexError: list index out of range\n",
    "\n",
    "Waiting all threads to end\n",
    "Finalizado\n",
    "1114242 entities, 1493 relations, 1437504 tripletas\n",
    "0:09:44.500492\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtset.save_to_binary(\"wdata1lvl.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114242 entities, 1493 relations, 1437504 tripletas\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dataset = dataset.Dataset()\n",
    "\n",
    "\n",
    "#dataset.load_entire_dataset(1)\n",
    "dataset.load_from_binary(\"wikidata_1level.bin\")\n",
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times= [10, 60, 120, 180, 300, 420, 500, 600, 700, 800, 900]\n",
    "try_catch_all = [548, 2635, None, 7216, 12142, 15622, None, 20500, 23986, 28086, 31591]\n",
    "if_else_all=[550, 3369, 6771, 10057, 16666, 23575, 28325, None, None, None]\n",
    "if_else_all2 = [818,3654,6924,17093,None,28298,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
