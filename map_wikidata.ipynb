{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "def get_bne_wikidata(max_number=100):\n",
    "    key = \"bne\"\n",
    "    value = \"wikidata\"\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "    SELECT ?bne ?wikidata\n",
    "    WHERE { \n",
    "        ?wikidata wdt:P950 ?bne .\n",
    "    }\n",
    "    LIMIT \"\"\"+str(max_number)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return [(item[key]['value'], item[value]['value']) for item in json_data]\n",
    "\n",
    "\n",
    "datos = get_bne_wikidata()\n",
    "#for key,value in datos:\n",
    "    #print(key,\"\\t\",value)\n",
    "\n",
    "f = open(\"wikidatamap.bin\", \"wb+\")\n",
    "pickle.dump(datos, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "query2 = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct { ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }\n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "    ?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "} LIMIT 10000 OFFSET 1880000\n",
    "\"\"\"\n",
    "datos.load_dataset_from_query(query2, only_uri=False)\n",
    "\n",
    "\n",
    "#datos.load_dataset_from_nlevels(5, extra_params=\"LIMIT 100\")\n",
    "datos.show(verbose=False)\n",
    "dataset1 = datos.train_split()\n",
    "\n",
    "#     print(sub,dataset1[sub])\n",
    "datos.save_to_binary(\"wikidata_2.bin\")\n",
    "\n",
    "# datos.subs\n",
    "\n",
    "#print(len(jsonlist))\n",
    "#jsonlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rel = [i[2] for i in datos.subs]\n",
    "print(len(datos.relations_dict))\n",
    "print(len(datos.relations))\n",
    "print(len(datos.entities_dict))\n",
    "print(len(datos.entities))\n",
    "rel = np.array(rel)\n",
    "print(np.sort(rel))\n",
    "data = np.matrix(datos.subs)\n",
    "print(data[:,2])\n",
    "\n",
    "print('Splitting the data into training and validation sets ...')\n",
    "print(data.shape)\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "nb_validation_samples = int(0.2 * data.shape[0])\n",
    "\n",
    "\n",
    "x_train = data[:-nb_validation_samples]\n",
    "x_val = data[-nb_validation_samples:-int(nb_validation_samples/2)]\n",
    "x_test = data[-int(nb_validation_samples/2):]\n",
    "\n",
    "print(x_val.shape,\"\\n\\n\\n\",x_test.shape,\"\\n\\n\\n\",x_train.shape)\n",
    "print(x_test.shape[0]+x_train.shape[0]+x_val.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14951 entities, 1345 relations, 592213 tripletas\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "importlib.reload(dataset)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "fb_test = open(\"FB15k/freebase_mtr100_mte100-test.txt\", \"r\")\n",
    "fb_train = open(\"FB15k/freebase_mtr100_mte100-train.txt\", \"r\")\n",
    "fb_valid = open(\"FB15k/freebase_mtr100_mte100-valid.txt\", \"r\")\n",
    "datos.load_dataset_from_csv(fb_train, \"\\t\")\n",
    "datos.load_dataset_from_csv(fb_test, \"\\t\")\n",
    "datos.load_dataset_from_csv(fb_valid, \"\\t\")\n",
    "\n",
    "datos.show(verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/m/063k3h\n",
      "50\n",
      "/m/0739z6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/m/063k3h'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en = dataset.Entities(datos).get_entity(50)\n",
    "print(en)\n",
    "id = dataset.Entities(datos).get_id(\"/m/063k3h\")\n",
    "print(id)\n",
    "en2 = dataset.Entities(datos).get_entity(13299)\n",
    "print(en2)\n",
    "datos.entities.index(\"/m/063k3h\")\n",
    "datos.entities[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_json_offset(n, count):\n",
    "    endpoint = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"\"\"\n",
    "    \n",
    "    query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct {{ ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }}\n",
    "WHERE {{ ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "}} LIMIT {0} OFFSET {1}\n",
    "\"\"\".format(count, n*count)\n",
    "    \n",
    "    headers = {\"Accept\" : \"application/json\"}\n",
    "    response = requests.get(endpoint+query, headers=headers)\n",
    "    if response.status_code is not 200:\n",
    "        return False, \"Error occurred on http request. Code\"+str(response.status_code)\n",
    "    json_data = response.json()['results']['bindings']\n",
    "    return json_data\n",
    "\n",
    "all_json = []\n",
    "for query in range(0,30):\n",
    "    json1 = get_json_offset(query,20000)\n",
    "    all_json = all_json + json1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import queries\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(queries)\n",
    "\n",
    "query = queries.Queries()\n",
    "\n",
    "js = query.big_query(n_levels=3, n_rounds=4, total=10000)\n",
    "print(\"finished. Saving\")\n",
    "f1 = open(\"millon.json\", \"wb+\")\n",
    "pickle.dump(js, f1)\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import kgeserver.dataset as dataset\n",
    "import kgeserver.wikidata_dataset as wikidata_dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(wikidata_dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dtset = wikidata_dataset.WikidataDataset()\n",
    "\n",
    "\n",
    "#dataset.load_entire_dataset(1)\n",
    "dtset.load_dataset_recurrently(3, verbose=2, limit_ent=500)\n",
    "dtset.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P31\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dtset.relations[8])\n",
    "dtset.save_to_binary(\"wikidata_25k.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import algorithm\n",
    "import skge\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dt = dataset.Dataset()\n",
    "\n",
    "dt.load_from_binary(\"wn18.bin\")\n",
    "\n",
    "al = algorithm.Algorithm(dt)\n",
    "\n",
    "models = models = al.find_best(ncomps=[50], margins=[2], model_types=[skge.TransE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import dataset\n",
    "import algorithm\n",
    "import skge\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dt = dataset.Dataset()\n",
    "\n",
    "dt.load_from_binary(\"wn18.bin\")\n",
    "\n",
    "al = algorithm.Algorithm(dt)\n",
    "\n",
    "models = models = al.find_best(ncomps=[150], margins=[0.2], model_types=[skge.HolE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None None\n",
      "None None\n",
      "None None\n",
      "P159 Q62\n",
      "None None\n",
      "None None\n",
      "None None\n",
      "None None\n",
      "['Q62']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Q62']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def extract_triples_from_statement(uri):\n",
    "    st_query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "      SELECT ?pred ?subj\n",
    "      WHERE {{\n",
    "      <{0}> ?pred ?subj .\n",
    "      }}\"\"\".format(uri)\n",
    "    response = requests.get(\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\"+st_query, headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "    el_json = response.json()[\"results\"][\"bindings\"]\n",
    "\n",
    "    # Check errors\n",
    "#     if sts is not 200:\n",
    "#         return None\n",
    "    el_queue = []\n",
    "    \n",
    "    for el in el_json:\n",
    "        pr = (el['pred']['value'])        \n",
    "        sj = (el['subj']['value'])\n",
    "        sj = check_entity(sj)\n",
    "        pr = check_relation(pr)\n",
    "        if sj:\n",
    "            el_queue.append(sj)\n",
    "\n",
    "   \n",
    "    return el_queue\n",
    "extract_triples_from_statement(\"http://www.wikidata.org/entity/statement/q180-BD4C0870-A565-4952-BF6E-F435F7ACF615\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P2853\n",
      "P159\n",
      "None\n",
      "None\n",
      "Q62\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def check_entity(entity):\n",
    "    \"\"\"Check the entity given and return a valid representation\n",
    "\n",
    "    :param string entity: The input entity representation\n",
    "    :return: A valid representation or None\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    # This expects as input an entity URI\n",
    "    # http://www.wikidata.org/entity/Q180\n",
    "    try:\n",
    "        # If either fails to convert the last Q number into int\n",
    "        # or the URI hasn't 'entity' keyword, returns without doing nothing\n",
    "        ent_uri = entity.split(\"/\")\n",
    "        wikidata_id = ent_uri[-1]\n",
    "        # print(entity, wikidata_id)\n",
    "        if ent_uri[-2] == 'entity' and type(int(wikidata_id[1:])) is int and wikidata_id[0] is \"Q\":\n",
    "            return wikidata_id\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def check_relation( relation):\n",
    "    \"\"\"Check the relation given and return a valid representation\n",
    "\n",
    "    :param string relation: The input relation representation\n",
    "    :return: A valid representation or None\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    # http://www.wikidata.org/prop/direct/P2853 VALID\n",
    "    # http://www.wikidata.org/prop/statement/P159 VALID\n",
    "    # http://www.wikidata.org/prop/qualifier/P18 NOT VALID\n",
    "    try:\n",
    "        # If either fails to convert the last Q number into int\n",
    "        # or the URI hasn't 'entity' keyword, returns without doing nothing\n",
    "        prp_uri = relation.split(\"/\")\n",
    "        wikidata_prop = prp_uri[-1]\n",
    "        if wikidata_prop[0] is \"P\" and prp_uri[-3] == 'prop' and (prp_uri[-2] == \"direct\" or prp_uri[-2] == \"statement\") and type(int(wikidata_prop[1:])) is int:\n",
    "            return wikidata_prop\n",
    "        else:\n",
    "            return None\n",
    "    except Exception:\n",
    "        print(\"ex\")\n",
    "        return None\n",
    "    return None\n",
    "print(check_relation(\"http://www.wikidata.org/prop/direct/P2853\"))\n",
    "print(check_relation(\"http://www.wikidata.org/prop/statement/P159\"))\n",
    "print(check_relation(\"http://www.wikidata.org/prop/qualifier/P18\"))\n",
    "print(check_entity(\"http://www.wikidata.org/value/db2cef2363893c6df38812b8c3b38263\"))\n",
    "print(check_entity(\"http://www.wikidata.org/entity/Q62\"))\n",
    "print(check_entity(\"http://commons.wikimedia.org/wiki/Special:FilePath/New%20Wikimedia%20Foundation%20Office%2014.jpg/Q34\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
