{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urlRequest = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\n",
    "PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "SELECT ?bne ?wikidata \n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "}\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "headers = {\"Accept\" : \"application/json\"}\n",
    "response = requests.get(urlRequest, headers=headers)\n",
    "\n",
    "\n",
    "map_bne_wkdt = [(item['bne']['value'],item['wikidata']['value']) for item in response.json()['results']['bindings']]\n",
    "# Resultado deseado (BNE-ID, Wikidata-Entity-URI)\n",
    "\n",
    "\n",
    "entities = []\n",
    "relations = []\n",
    "subs = []\n",
    "\n",
    "def add_element(element, complete_list):\n",
    "    try:\n",
    "        #Item is on the list -> you don't need to add again\n",
    "        return complete_list.index(element)\n",
    "    except ValueError:\n",
    "        # Item is not on the list -> add it and return id\n",
    "        complete_list.append(element)\n",
    "        return len(complete_list)-1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct { ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }\n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def build_n_levels_query(n_levels):\n",
    "    def build_levels(n_levels):\n",
    "    ob1 = \"wikidata\"\n",
    "    pre = \"predicate\"\n",
    "    ob2 = \"object\"\n",
    "    pre_base = pre\n",
    "    obj_base = ob2\n",
    "    predicateCount = 1\n",
    "    objectCount = 1\n",
    "    \n",
    "    tripletas = []\n",
    "    \n",
    "    for level in range(1,n_levels+1):\n",
    "        tripletas.append((ob1, pre, ob2))\n",
    "        objectCount += 1\n",
    "        predicateCount += 1\n",
    "        ob1 = ob2\n",
    "        ob2 = obj_base + str(objectCount)\n",
    "        pre = pre_base + str(predicateCount)\n",
    "    \n",
    "    return tripletas\n",
    "\n",
    "\n",
    "lines = []\n",
    "for level in build_levels(4):\n",
    "    lines.append(\"?\"+level[0]+\" ?\"+level[0]+\" ?\"+level[2])\n",
    "    \n",
    "query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct {{ {0} }}\n",
    "WHERE {{ ?wikidata wdt:P950 ?bne .\n",
    "{1}\n",
    "}} \"\"\".format(\" . \".join(lines), \" . \\n\".join(lines))\n",
    "\n",
    "print(query)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"holographic-embeddings/modeloentrenado\", 'rb')\n",
    "t = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict as ddict\n",
    "idx = ddict(list)\n",
    "tt = ddict(lambda: {'ss': ddict(list), 'os': ddict(list)})\n",
    "xs = [(1,2,3),(3,2,5),(1,2,4),(4,6,3)]\n",
    "for s, p, o in xs:\n",
    "    idx[p].append((s,o))\n",
    "\n",
    "for s, p, o in xs:\n",
    "    tt[p]['os'][s].append(o)\n",
    "    tt[p]['ss'][o].append(s)\n",
    "    \n",
    "print(dict(tt))\n",
    "# print(idx)\n",
    "print(dict(idx).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [(\"a\",[(0.026937118701835055, 50), (0.026937118701835055, 50)]), (\"b\",[(0.00099489368693191972, 50), (0.00099489368693191972, 50)])]\n",
    "sorted(l, key=lambda t: t[1][0], reverse=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "import algorithm\n",
    "import experiment\n",
    "import skge\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dtset = dataset.Dataset()\n",
    "# dataset.load_from_binary(\"holographic-embeddings/data/wn18.bin\")\n",
    "dtset.load_from_binary(\"wikidata_25k.bin\")\n",
    "\n",
    "alg = algorithm.Algorithm(dtset)\n",
    "all_models = alg.find_best(ncomps=range(80, 100, 20),model_types=[skge.TransE])\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = all_models[2].run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.E\n",
    "import pickle\n",
    "f = open(\"modeltrained-1.bin\", \"wb+\")\n",
    "pickle.dump(a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"skge/\")\n",
    "import skge\n",
    "import experiment\n",
    "import importlib\n",
    "import pickle\n",
    "import kgeserver.dataset as dataset\n",
    "import kgeserver.algorithm as algorithm\n",
    "import kgeserver.experiment as experiment\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dtset = dataset.Dataset()\n",
    "# dataset.load_from_binary(\"holographic-embeddings/data/wn18.bin\")\n",
    "dtset.load_from_binary(\"wikidata_25k.bin\")\n",
    "\n",
    "alg = algorithm.Algorithm(dtset)\n",
    "all_models = alg.find_best(ncomps=range(80, 100, 20), model_types=[skge.HolE], margins=[2])\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 entities, 1 relations, 2 tripletas\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, 3, 0), (0, 1, 0)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kgeserver.dataset as dataset\n",
    "import json\n",
    "import importlib\n",
    "importlib.reload(dataset)\n",
    "\n",
    "dtset = dataset.Dataset()\n",
    "dtset.load_from_binary(\"1477651621.bin\")\n",
    "# js = json.loads('{\"triples\": [{\"subject\":\"Q1492\", \"predicate\":\"P17\", \"object\":\"Q29\"}]}')\n",
    "# dtset.load_dataset_from_json(js['triples'])\n",
    "dtset.show()\n",
    "# dtset.save_to_binary(\"1477647842.bin\")\n",
    "dtset.subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "matriz = np.empty([])\n",
    "print(matriz.shape)\n",
    "# np.arange(matriz.shape[0])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
