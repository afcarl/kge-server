{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urlRequest = \"\"\"https://query.wikidata.org/bigdata/namespace/wdq/sparql?query=\n",
    "PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "SELECT ?bne ?wikidata \n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "}\n",
    "LIMIT 30\n",
    "\"\"\"\n",
    "\n",
    "headers = {\"Accept\" : \"application/json\"}\n",
    "response = requests.get(urlRequest, headers=headers)\n",
    "\n",
    "\n",
    "map_bne_wkdt = [(item['bne']['value'],item['wikidata']['value']) for item in response.json()['results']['bindings']]\n",
    "# Resultado deseado (BNE-ID, Wikidata-Entity-URI)\n",
    "\n",
    "\n",
    "entities = []\n",
    "relations = []\n",
    "subs = []\n",
    "\n",
    "def add_element(element, complete_list):\n",
    "    try:\n",
    "        #Item is on the list -> you don't need to add again\n",
    "        return complete_list.index(element)\n",
    "    except ValueError:\n",
    "        # Item is not on the list -> add it and return id\n",
    "        complete_list.append(element)\n",
    "        return len(complete_list)-1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct { ?wikidata ?predicate ?object . ?object ?predicate2 ?object2 . ?object2 ?predicate3 ?object3 }\n",
    "WHERE { ?wikidata wdt:P950 ?bne .\n",
    "?wikidata ?predicate ?object .\n",
    "?object ?predicate2 ?object2 .\n",
    "?object2 ?predicate3 ?object3\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def build_n_levels_query(n_levels):\n",
    "    def build_levels(n_levels):\n",
    "    ob1 = \"wikidata\"\n",
    "    pre = \"predicate\"\n",
    "    ob2 = \"object\"\n",
    "    pre_base = pre\n",
    "    obj_base = ob2\n",
    "    predicateCount = 1\n",
    "    objectCount = 1\n",
    "    \n",
    "    tripletas = []\n",
    "    \n",
    "    for level in range(1,n_levels+1):\n",
    "        tripletas.append((ob1, pre, ob2))\n",
    "        objectCount += 1\n",
    "        predicateCount += 1\n",
    "        ob1 = ob2\n",
    "        ob2 = obj_base + str(objectCount)\n",
    "        pre = pre_base + str(predicateCount)\n",
    "    \n",
    "    return tripletas\n",
    "\n",
    "\n",
    "lines = []\n",
    "for level in build_levels(4):\n",
    "    lines.append(\"?\"+level[0]+\" ?\"+level[0]+\" ?\"+level[2])\n",
    "    \n",
    "query = \"\"\"PREFIX wikibase: <http://wikiba.se/ontology>\n",
    "construct {{ {0} }}\n",
    "WHERE {{ ?wikidata wdt:P950 ?bne .\n",
    "{1}\n",
    "}} \"\"\".format(\" . \".join(lines), \" . \\n\".join(lines))\n",
    "\n",
    "print(query)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict1 = {\"a\":1, \"b\":2, \"c\":3, \"d\":4}\n",
    "print(dict1[\"b\"])\n",
    "print(len(dict1))\n",
    "dict1[\"e\"]=5\n",
    "len(dict1)\n",
    "l = [[3,4,5],[3,2,5],[3,5,5]]\n",
    "[tuple(x) for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "import model\n",
    "import skge\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(model)\n",
    "\n",
    "\n",
    "datos = dataset.Dataset()\n",
    "datos.load_from_binary(\"holographic-embeddings/data/wikidata_2.bin\")\n",
    "datos.show(verbose=False)\n",
    "\n",
    "tr = model.AutoModel(datos)\n",
    "model = tr.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "f = open(\"holographic-embeddings/modeloentrenado\", 'rb')\n",
    "t = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict as ddict\n",
    "idx = ddict(list)\n",
    "tt = ddict(lambda: {'ss': ddict(list), 'os': ddict(list)})\n",
    "xs = [(1,2,3),(3,2,5),(1,2,4),(4,6,3)]\n",
    "for s, p, o in xs:\n",
    "    idx[p].append((s,o))\n",
    "\n",
    "for s, p, o in xs:\n",
    "    tt[p]['os'][s].append(o)\n",
    "    tt[p]['ss'][o].append(s)\n",
    "    \n",
    "print(dict(tt))\n",
    "# print(idx)\n",
    "print(dict(idx).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = [(\"a\",[(0.026937118701835055, 50), (0.026937118701835055, 50)]), (\"b\",[(0.00099489368693191972, 50), (0.00099489368693191972, 50)])]\n",
    "sorted(l, key=lambda t: t[1][0], reverse=True):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "import algorithm\n",
    "import experiment\n",
    "import skge\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dtset = dataset.Dataset()\n",
    "# dataset.load_from_binary(\"holographic-embeddings/data/wn18.bin\")\n",
    "dtset.load_from_binary(\"wikidata_25k.bin\")\n",
    "\n",
    "alg = algorithm.Algorithm(dtset)\n",
    "all_models = alg.find_best(ncomps=range(80, 100, 20),model_types=[skge.TransE])\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "import algorithm\n",
    "import experiment\n",
    "import skge\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "dtset = dataset.Dataset()\n",
    "# dataset.load_from_binary(\"holographic-embeddings/data/wn18.bin\")\n",
    "dtset.load_from_binary(\"wikidata_60k.bin\")\n",
    "\n",
    "alg = algorithm.Algorithm(dtset)\n",
    "models = alg.find_best(ncomps=range(10, 100, 20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = all_models[2].run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.E\n",
    "import pickle\n",
    "f = open(\"modeltrained-1.bin\", \"wb+\")\n",
    "pickle.dump(a, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q16562\n",
      "http://www.wikidata.org/entity/Q1520\n",
      "http://www.wikidata.org/entity/Q3640\n",
      "http://www.wikidata.org/entity/Q2096\n",
      "http://www.wikidata.org/entity/Q16568\n",
      "http://www.wikidata.org/entity/Q9899\n",
      "http://www.wikidata.org/entity/Q132790\n",
      "http://www.wikidata.org/entity/Q1000346\n",
      "http://www.wikidata.org/entity/Q290504\n",
      "http://www.wikidata.org/entity/Q1435\n"
     ]
    }
   ],
   "source": [
    "arr = [13215, 12075, 2823, 9052, 13635, 9410, 2681, 13218, 12981, 13519]\n",
    "for e in arr:\n",
    "    print(dataset.Entities(dtset).get_entity(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "# import annoy\n",
    "from annoy import AnnoyIndex\n",
    "# import random\n",
    "# import pickle\n",
    "\n",
    "# trained = open(\"holographic-embeddings/modeloentrenado\", \"rb\")\n",
    "# trained_m = pickle.load(trained)\n",
    "\n",
    "# e_matrix = trained_m['model'].E\n",
    "e_matrix = a.E\n",
    "nrows, nmod = e_matrix.shape\n",
    "\n",
    "t = AnnoyIndex(nmod)\n",
    "for i in range(0, nrows):\n",
    "    v = list(e_matrix[i])\n",
    "    t.add_item(i, v)\n",
    "\n",
    "t.build(1000)\n",
    "t.save('test.ann')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13215, 12075, 2823, 9052, 13635, 9410, 2681, 13218, 12981, 13519]\n"
     ]
    }
   ],
   "source": [
    "print(t.get_nns_by_item(13215, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = pickle.load(open(\"modeltrained-1.bin\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"modeltrained-1.bin\", \"rb\")\n",
    "a = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<server.SearchIndex object at 0x7fe16af0e400>\n"
     ]
    }
   ],
   "source": [
    "import server\n",
    "import importlib\n",
    "importlib.reload(server)\n",
    "\n",
    "si = server.SearchIndex()\n",
    "si.build_from_trained_model(a, 100)\n",
    "print(si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import server\n",
    "import importlib\n",
    "importlib.reload(server)\n",
    "s = server.Server(si)\n",
    "s.similarity_by_id(4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"skge/\")\n",
    "import skge\n",
    "import experiment\n",
    "import importlib\n",
    "import pickle\n",
    "import dataset\n",
    "import algorithm\n",
    "import experiment\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "\n",
    "\n",
    "dtset = dataset.Dataset()\n",
    "# dataset.load_from_binary(\"holographic-embeddings/data/wn18.bin\")\n",
    "dtset.load_from_binary(\"wikidata_25k.bin\")\n",
    "\n",
    "alg = algorithm.Algorithm(dtset)\n",
    "all_models = alg.find_best(ncomps=range(80, 100, 20),model_types=[skge.HolE], margins=[2])\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def check_relation(relation):\n",
    "    \"\"\"Check the relation given and return a valid representation\n",
    "\n",
    "    :param string relation: The input relation representation\n",
    "    :return: A valid representation or None\n",
    "    :rtype: string\n",
    "    \"\"\"\n",
    "    # http://www.wikidata.org/prop/direct/P2853\n",
    "    try:\n",
    "        # If either fails to convert the last Q number into int\n",
    "        # or the URI hasn't 'entity' keyword, returns without doing nothing\n",
    "        wikidata_prop = relation.split(\"/\")[-1]\n",
    "\n",
    "        if not relation.split(\"/\")[-3] == 'prop' and\\\n",
    "                type(int(wikidata_prop[1:])) is int and\\\n",
    "                wikidata_prop[1] is not \"P\":\n",
    "            return None\n",
    "        else:\n",
    "            return wikidata_prop\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "\n",
    "check_relation(\"http://www.wikidata.org/entity/Q180\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'pypy'\n"
     ]
    }
   ],
   "source": [
    "dict_ = {}\n",
    "di = {\"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/a\":4, \"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/b\": 3}\n",
    "strin = \"http://www.wikidata.org/prop/direct/P{0}\"\n",
    "\n",
    "for i in range(0, 1000000):\n",
    "    s = strin.format(i)\n",
    "    dict_[s] = i\n",
    "    \n",
    "lis = [strin.format(i) for i in range(0, 1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 14.26 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000000 loops, best of 3: 101 ns per loop\n",
      "The slowest run took 10.24 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000000 loops, best of 3: 118 ns per loop\n",
      "10 loops, best of 3: 26.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dict_[\"http://www.wikidata.org/prop/direct/P99334\"]\n",
    "%timeit di[\"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/b\"]\n",
    "%timeit lis.index(\"http://www.wikidata.org/prop/direct/P999994\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ExecuteQueryError",
     "evalue": "An error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExecuteQueryError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-cc50806d3cbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mExecuteQueryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"An error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mExecuteQueryError\u001b[0m: An error"
     ]
    }
   ],
   "source": [
    "class ExecuteQueryError(Exception):\n",
    "    def __init__(self, message):\n",
    "        super(ExecuteQueryError, self).__init__(message)\n",
    "\n",
    "\n",
    "\n",
    "raise ExecuteQueryError(\"An error\")\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
