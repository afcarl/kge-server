{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holographic embeddings data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset uses pickle to store and save on disk. When loaded from python, it comes a dictionary structure wich has 5 different entries:\n",
    "\n",
    "* `entities`\n",
    "* `relations`\n",
    "* And three different subsets\n",
    "    * `test_subs`\n",
    "    * `valid_subs`\n",
    "    * `train_subs`\n",
    "\n",
    "The **`entities`** and **`relations`** list contains simply identifiers. The three different subsets contains the relations between `entities` and `relations`, divided in `train_subs`, which is the biggest subset, `test_subs` and `valid_subs`. The structure of this subset is the same: [(`<id_entity>`, `<id_entity>`, `<id_relation>`), ...]. The id's are exactly the position they occupy on the entity and relations array, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('train_subs', 7997), ('relations', 930), ('entities', 8363), ('valid_subs', 1000), ('test_subs', 999)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/home/jovyan/holographic-embeddings/data/wn18.bin\", 'rb') as fin:\n",
    "    data = pickle.load(fin)\n",
    "print ([(k, len(data[k])) for k in data.keys()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As an example, we can see below the internal structure of `valid_subs`. Is a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7224, 456, 243),\n",
       " (3710, 128, 38),\n",
       " (1760, 227, 109),\n",
       " (4747, 1097, 265),\n",
       " (1500, 245, 332),\n",
       " (2846, 147, 37),\n",
       " (1710, 245, 359),\n",
       " (3579, 226, 261),\n",
       " (5937, 802, 261),\n",
       " (7911, 806, 382)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tupla for tupla in data['valid_subs'][0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset python class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main target is having a Dataset class which can be filled with external data, and this object can be saved on disk with the same structure used on https://github.com/mnick/holographic-embeddings to train a network.\n",
    "\n",
    "Right now, the class has several methods:\n",
    "* `load_dataset_from_json`\n",
    "* `load_dataset_from_query`\n",
    "* `load_dataset_from_nlevels`\n",
    "* `load_entire_dataset`\n",
    "* `save_to_binary`\n",
    "* `load_from_binary`\n",
    "* `train_split`\n",
    "* `show`\n",
    "* And other private methods.\n",
    "\n",
    "On the `load_entire_dataset` method. It is necessary to generate internally a count query in order to know how many tuples should be retrieved from server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32871 entities\n",
      "Scanning level 1 with 33009 elements\n",
      "Enter S to show status: s\n",
      "Elapsed time: 5s. Depth 1 of 2. Entities scanned: 1.35% (446 of 33009) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 52s. Depth 1 of 2. Entities scanned: 9.81% (3238 of 33009) Active threads: 29\n",
      "Waiting all threads to end\n",
      "Scanning level 2 with 366492 elements\n",
      "Waiting all threads to end\n",
      "220656 entities, 516 relations, 873012 tripletas\n",
      "Enter S to show status: s\n",
      "Elapsed time: 1538s. Depth 2 of 2. Entities scanned: 100.00% (366482 of 366492) Active threads: 6\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import kgeserver.dataset as dataset\n",
    "import kgeserver.wikidata_dataset as wikidata_dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(wikidata_dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dtset = wikidata_dataset.WikidataDataset()\n",
    "\n",
    "sv = dtset.get_seed_vector(verbose=2)\n",
    "#dataset.load_entire_dataset(1)\n",
    "dtset.load_dataset_recurrently(2, sv, verbose=2)\n",
    "dtset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Taking a dict as key, value storage is much faster than try to save long strings in arrays. \n",
    "Also, the search is faster on a dict than in a list. The search is even faster when shorter is the string\n",
    "used as key in dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "di2 = {}\n",
    "di = {\"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/a\": 4,\n",
    "      \"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/b\": 3}\n",
    "\n",
    "st = \"P{0}\"\n",
    "strin = \"http://www.wikidata.org/prop/direct/P{0}\"\n",
    "\n",
    "\n",
    "for i in range(0, 1000000):\n",
    "    s = strin.format(i)\n",
    "    dict_[s] = i\n",
    "    di2[st.format(i)] = i\n",
    "    \n",
    "lis = [strin.format(i) for i in range(0, 1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 38.21 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000000 loops, best of 3: 106 ns per loop\n",
      "10000000 loops, best of 3: 119 ns per loop\n",
      "The slowest run took 13.65 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000000 loops, best of 3: 77.2 ns per loop\n",
      "10 loops, best of 3: 26.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dict_[\"http://www.wikidata.org/prop/direct/P999994\"]\n",
    "%timeit di[\"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/b\"]\n",
    "%timeit di2[\"P999994\"]\n",
    "%timeit lis.index(\"http://www.wikidata.org/prop/direct/P999994\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage for Algorithm class\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "import kgeserver.dataset as dataset\n",
    "import kgeserver.algorithm as algorithm\n",
    "import kgeserver.experiment as experiment\n",
    "import skge\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "dtset = dataset.Dataset()\n",
    "# dataset.load_from_binary(\"holographic-embeddings/data/wn18.bin\")\n",
    "dtset.load_from_binary(\"wikidata_25k.bin\")\n",
    "\n",
    "#alg = algorithm.Algorithm(dtset, thread_limiter=5)\n",
    "model = algorithm.ModelTrainer(dtset, model_type=skge.TransE, margin=0.2, ncomp=100, test_all=-1, train_all=True)\n",
    "#models = alg.find_best(ncomps=[100], model_types=[skge.TransE], test_all=-1, train_all=True, margins = [0.2])\n",
    "modelo = model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kgeserver.server as server\n",
    "si = server.SearchIndex()\n",
    "si.build_from_trained_model(modelo, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si.save_to_binary(\"wikidata_25k.annoy.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage for Server Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a SearchIndex. We can choose between create a new one from a trained model, or load from other already built.\n",
    "\n",
    "The Dataset Class is loaded because is useful to work with entities' strings and id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kgeserver.server as server\n",
    "import kgeserver.dataset as dataset\n",
    "import pickle\n",
    "\n",
    "si = server.SearchIndex()\n",
    "\n",
    "# tm = pickle.load(open(\"modeloentrenado100k.bin\", \"rb\"))\n",
    "# si.build_from_trained_model(tm, 1000)\n",
    "\n",
    "# si.save_to_binary(\"annoyIndex100k.bin\")\n",
    "si.load_from_file(\"annoy_index_big.bin\", 100)\n",
    "\n",
    "dt = dataset.Dataset()\n",
    "dt.load_from_binary(\"4levels.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the server class. Is as simple as instantiate a Server object with the searchIndex attribute.\n",
    "\n",
    "In the example, gets a similar entities vector from a given id, and shows the complete URI through screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    458, 0.0000] https://www.wikidata.org/wiki/Q217\n",
      "[  97806, 0.8554] https://www.wikidata.org/wiki/Q907112\n",
      "[    391, 0.8634] https://www.wikidata.org/wiki/Q800\n",
      "[    392, 0.8813] https://www.wikidata.org/wiki/Q801\n",
      "[    381, 0.8813] https://www.wikidata.org/wiki/Q734\n",
      "[  97821, 0.8823] https://www.wikidata.org/wiki/Q865726\n",
      "[  97811, 0.8834] https://www.wikidata.org/wiki/Q266444\n",
      "[    438, 0.8836] https://www.wikidata.org/wiki/Q43\n",
      "[  97809, 0.8980] https://www.wikidata.org/wiki/Q164819\n",
      "[    287, 0.8997] https://www.wikidata.org/wiki/Q1032\n",
      "[  63110, 0.9029] https://www.wikidata.org/wiki/Q21197\n",
      "[  75687, 0.9120] https://www.wikidata.org/wiki/Q837\n",
      "[  24585, 0.9127] https://www.wikidata.org/wiki/Q236\n",
      "[  97841, 0.9197] https://www.wikidata.org/wiki/Q2129025\n",
      "[    288, 0.9220] https://www.wikidata.org/wiki/Q1033\n",
      "[    455, 0.9234] https://www.wikidata.org/wiki/Q212\n",
      "[    456, 0.9247] https://www.wikidata.org/wiki/Q218\n",
      "[  94172, 0.9276] https://www.wikidata.org/wiki/Q846\n",
      "[  97842, 0.9284] https://www.wikidata.org/wiki/Q2129061\n",
      "[    380, 0.9292] https://www.wikidata.org/wiki/Q730\n",
      "[    472, 0.9303] https://www.wikidata.org/wiki/Q241\n",
      "[    254, 0.9312] https://www.wikidata.org/wiki/Q884\n",
      "[  68896, 0.9330] https://www.wikidata.org/wiki/Q813\n",
      "[    427, 0.9352] https://www.wikidata.org/wiki/Q32\n",
      "[  97813, 0.9360] https://www.wikidata.org/wiki/Q611656\n",
      "[    370, 0.9395] https://www.wikidata.org/wiki/Q403\n",
      "[    331, 0.9405] https://www.wikidata.org/wiki/Q458\n",
      "[  13734, 0.9410] https://www.wikidata.org/wiki/Q754\n",
      "[  97831, 0.9411] https://www.wikidata.org/wiki/Q1796621\n",
      "[  97822, 0.9440] https://www.wikidata.org/wiki/Q865884\n"
     ]
    }
   ],
   "source": [
    "import kgeserver.server as server\n",
    "import importlib\n",
    "importlib.reload(server)\n",
    "\n",
    "id1 = dt.get_entity_id(\"Q83186\")\n",
    "\n",
    "s = server.Server(si)\n",
    "simil = s.similarity_by_id(458,30)\n",
    "for ent, dist in simil:\n",
    "    b_ent = \"https://www.wikidata.org/wiki/\"+dt.get_entity(ent)\n",
    "    print(\"[{0:7d}, {1:2.04f}] {2}\".format(ent, dist, b_ent))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
