{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holographic embeddings data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset uses pickle to store and save on disk. When loaded from python, it comes a dictionary structure wich has 5 different entries:\n",
    "\n",
    "* `entities`\n",
    "* `relations`\n",
    "* And three different subsets\n",
    "    * `test_subs`\n",
    "    * `valid_subs`\n",
    "    * `train_subs`\n",
    "\n",
    "The **`entities`** and **`relations`** list contains simply identifiers. The three different subsets contains the relations between `entities` and `relations`, divided in `train_subs`, which is the biggest subset, `test_subs` and `valid_subs`. The structure of this subset is the same: [(`<id_entity>`, `<id_entity>`, `<id_relation>`), ...]. The id's are exactly the position they occupy on the entity and relations array, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('train_subs', 7997), ('relations', 930), ('entities', 8363), ('valid_subs', 1000), ('test_subs', 999)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"/home/jovyan/holographic-embeddings/data/wn18.bin\", 'rb') as fin:\n",
    "    data = pickle.load(fin)\n",
    "print ([(k, len(data[k])) for k in data.keys()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As an example, we can see below the internal structure of `valid_subs`. Is a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7224, 456, 243),\n",
       " (3710, 128, 38),\n",
       " (1760, 227, 109),\n",
       " (4747, 1097, 265),\n",
       " (1500, 245, 332),\n",
       " (2846, 147, 37),\n",
       " (1710, 245, 359),\n",
       " (3579, 226, 261),\n",
       " (5937, 802, 261),\n",
       " (7911, 806, 382)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tupla for tupla in data['valid_subs'][0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset python class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main target is having a Dataset class which can be filled with external data, and this object can be saved on disk with the same structure used on https://github.com/mnick/holographic-embeddings to train a network.\n",
    "\n",
    "Right now, the class has several methods:\n",
    "* `load_dataset_from_json`\n",
    "* `load_dataset_from_query`\n",
    "* `load_dataset_from_nlevels`\n",
    "* `load_entire_dataset`\n",
    "* `save_to_binary`\n",
    "* `load_from_binary`\n",
    "* `train_split`\n",
    "* `show`\n",
    "* And other private methods.\n",
    "\n",
    "On the `load_entire_dataset` method. It is necessary to generate internally a count query in order to know how many tuples should be retrieved from server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32871 entities\n",
      "Scanning level 1 with 33009 elements\n",
      "Enter S to show status: s\n",
      "Elapsed time: 5s. Depth 1 of 2. Entities scanned: 1.35% (446 of 33009) Active threads: 38\n",
      "Enter S to show status: s\n",
      "Elapsed time: 52s. Depth 1 of 2. Entities scanned: 9.81% (3238 of 33009) Active threads: 29\n",
      "Waiting all threads to end\n",
      "Scanning level 2 with 366492 elements\n",
      "Waiting all threads to end\n",
      "220656 entities, 516 relations, 873012 tripletas\n",
      "Enter S to show status: s\n",
      "Elapsed time: 1538s. Depth 2 of 2. Entities scanned: 100.00% (366482 of 366492) Active threads: 6\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import kgeserver.dataset as dataset\n",
    "import kgeserver.wikidata_dataset as wikidata_dataset\n",
    "import pickle\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(wikidata_dataset)\n",
    "from datetime import datetime\n",
    "\n",
    "dtset = wikidata_dataset.WikidataDataset()\n",
    "\n",
    "sv = dtset.get_seed_vector(verbose=2)\n",
    "#dataset.load_entire_dataset(1)\n",
    "dtset.load_dataset_recurrently(2, sv, verbose=2)\n",
    "dtset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Taking a dict as key, value storage is much faster than try to save long strings in arrays. \n",
    "Also, the search is faster on a dict than in a list. The search is even faster when shorter is the string\n",
    "used as key in dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "di2 = {}\n",
    "di = {\"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/a\": 4,\n",
    "      \"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/b\": 3}\n",
    "\n",
    "st = \"P{0}\"\n",
    "strin = \"http://www.wikidata.org/prop/direct/P{0}\"\n",
    "\n",
    "\n",
    "for i in range(0, 1000000):\n",
    "    s = strin.format(i)\n",
    "    dict_[s] = i\n",
    "    di2[st.format(i)] = i\n",
    "    \n",
    "lis = [strin.format(i) for i in range(0, 1000000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 38.21 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000000 loops, best of 3: 106 ns per loop\n",
      "10000000 loops, best of 3: 119 ns per loop\n",
      "The slowest run took 13.65 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000000 loops, best of 3: 77.2 ns per loop\n",
      "10 loops, best of 3: 26.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit dict_[\"http://www.wikidata.org/prop/direct/P999994\"]\n",
    "%timeit di[\"http://www.wikidata.org/prop/direct/P/prop/direct/P/prop/direct/P/b\"]\n",
    "%timeit di2[\"P999994\"]\n",
    "%timeit lis.index(\"http://www.wikidata.org/prop/direct/P999994\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage for Algorithm class\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import importlib\n",
    "import kgeserver.dataset as dataset\n",
    "import kgeserver.algorithm as algorithm\n",
    "import kgeserver.experiment as experiment\n",
    "import skge\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(dataset)\n",
    "importlib.reload(algorithm)\n",
    "dtset = dataset.Dataset()\n",
    "# dataset.load_from_binary(\"holographic-embeddings/data/wn18.bin\")\n",
    "dtset.load_from_binary(\"wikidata_25k.bin\")\n",
    "\n",
    "#alg = algorithm.Algorithm(dtset, thread_limiter=5)\n",
    "model = algorithm.ModelTrainer(dtset, model_type=skge.TransE, margin=0.2, ncomp=100, test_all=-1, train_all=True)\n",
    "#models = alg.find_best(ncomps=[100], model_types=[skge.TransE], test_all=-1, train_all=True, margins = [0.2])\n",
    "modelo = model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kgeserver.server as server\n",
    "si = server.SearchIndex()\n",
    "si.build_from_trained_model(modelo, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "si.save_to_binary(\"wikidata_25k.annoy.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage for Server Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a SearchIndex. We can choose between create a new one from a trained model, or load from other already built.\n",
    "\n",
    "The Dataset Class is loaded because is useful to work with entities' strings and id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kgeserver.server as server\n",
    "import kgeserver.dataset as dataset\n",
    "import pickle\n",
    "\n",
    "si = server.SearchIndex()\n",
    "\n",
    "# tm = pickle.load(open(\"modeloentrenado100k.bin\", \"rb\"))\n",
    "# si.build_from_trained_model(tm, 1000)\n",
    "\n",
    "# si.save_to_binary(\"annoyIndex100k.bin\")\n",
    "si.load_from_file(\"annoyIndex.500.bin\", 100)\n",
    "# si.load_from_file(\"annoy_index_big.bin\", 100)\n",
    "\n",
    "dt = dataset.Dataset()\n",
    "dt.load_from_binary(\"4levels.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the server class. Is as simple as instantiate a Server object with the searchIndex attribute.\n",
    "\n",
    "In the example, gets a similar entities vector from a given id, and shows the complete URI through screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  57750, 0.0000] http://www.wikidata.org/entity/Q83186\n",
      "[  25451, 0.9784] http://www.wikidata.org/entity/Q654170\n",
      "[  63322, 0.9813] http://www.wikidata.org/entity/Q706768\n",
      "[ 289971, 0.9837] http://www.wikidata.org/entity/Q1768437\n",
      "[ 181417, 0.9855] http://www.wikidata.org/entity/Q1472110\n",
      "[  48556, 0.9855] http://www.wikidata.org/entity/Q565911\n",
      "[ 289935, 0.9862] http://www.wikidata.org/entity/Q217112\n",
      "[ 308338, 0.9868] http://www.wikidata.org/entity/Q579392\n",
      "[   3361, 0.9877] http://www.wikidata.org/entity/Q320363\n",
      "[ 208530, 0.9976] http://www.wikidata.org/entity/Q2089457\n",
      "[ 384116, 0.9977] http://www.wikidata.org/entity/Q769001\n",
      "[  27365, 0.9978] http://www.wikidata.org/entity/Q742080\n",
      "[ 338929, 0.9993] http://www.wikidata.org/entity/Q107914\n",
      "[  47883, 1.0006] http://www.wikidata.org/entity/Q1249536\n",
      "[ 325197, 1.0018] http://www.wikidata.org/entity/Q2649025\n",
      "[  86260, 1.0033] http://www.wikidata.org/entity/Q105624\n",
      "[ 186949, 1.0034] http://www.wikidata.org/entity/Q1764445\n",
      "[  73431, 1.0046] http://www.wikidata.org/entity/Q336012\n",
      "[  27364, 1.0049] http://www.wikidata.org/entity/Q705669\n",
      "[ 338941, 1.0049] http://www.wikidata.org/entity/Q334780\n",
      "[ 338943, 1.0057] http://www.wikidata.org/entity/Q4941\n",
      "[ 381998, 1.0065] http://www.wikidata.org/entity/Q630496\n",
      "[  52298, 1.0076] http://www.wikidata.org/entity/Q1195086\n",
      "[   9489, 1.0081] http://www.wikidata.org/entity/Q243556\n",
      "[  29507, 1.0083] http://www.wikidata.org/entity/Q3847128\n",
      "[ 258386, 1.0097] http://www.wikidata.org/entity/Q1104517\n",
      "[ 338938, 1.0098] http://www.wikidata.org/entity/Q309289\n",
      "[   3724, 1.0123] http://www.wikidata.org/entity/Q1219561\n",
      "[ 361492, 1.0150] http://www.wikidata.org/entity/Q17485699\n",
      "[ 208514, 1.0153] http://www.wikidata.org/entity/Q699559\n"
     ]
    }
   ],
   "source": [
    "import kgeserver.server as server\n",
    "import importlib\n",
    "importlib.reload(server)\n",
    "\n",
    "id1 = dt.get_entity_id(\"Q83186\")\n",
    "\n",
    "s = server.Server(si)\n",
    "simil = s.similarity_by_id(id1,10)\n",
    "for ent, dist in simil:\n",
    "    b_ent = dt.get_entity(ent)\n",
    "    print(\"[{0:7d}, {1:2.04f}] {2}\".format(ent, dist, b_ent))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  57750, 0.7863] http://www.wikidata.org/entity/Q83186\n",
      "[  57660, 0.9696] http://www.wikidata.org/entity/Q1216733\n",
      "[ 208514, 0.9745] http://www.wikidata.org/entity/Q699559\n",
      "[ 136117, 0.9746] http://www.wikidata.org/entity/Q21006896\n",
      "[  60069, 0.9827] http://www.wikidata.org/entity/Q289484\n",
      "[ 256170, 0.9856] http://www.wikidata.org/entity/Q16290\n",
      "[ 161952, 0.9952] http://www.wikidata.org/entity/Q3982867\n",
      "[ 160393, 0.9962] http://www.wikidata.org/entity/Q7727273\n",
      "[  81303, 0.9992] http://www.wikidata.org/entity/Q5710913\n",
      "[ 170618, 1.0084] http://www.wikidata.org/entity/Q2421441\n"
     ]
    }
   ],
   "source": [
    "import kgeserver.server as server\n",
    "import importlib\n",
    "import pickle\n",
    "importlib.reload(server)\n",
    "\n",
    "id1 = dt.get_entity_id(\"Q83186\")\n",
    "\n",
    "modelo = pickle.load(open(\"modelo_guardado.bin\", \"rb\")).E\n",
    "espanya = modelo[dt.get_entity_id(\"Q29\")]\n",
    "paris = modelo[dt.get_entity_id(\"Q90\")]\n",
    "francia = modelo[dt.get_entity_id(\"Q142\")]\n",
    "reinounido = modelo[dt.get_entity_id(\"Q145\")]\n",
    "catalunya = modelo[dt.get_entity_id(\"Q5705\")]\n",
    "andalucia = modelo[dt.get_entity_id(\"Q5783\")]\n",
    "barcelona = modelo[dt.get_entity_id(\"Q1492\")]\n",
    "madrid = modelo[dt.get_entity_id(\"Q2807\")]\n",
    "toledo = modelo[dt.get_entity_id(\"Q5836\")]\n",
    "clm = modelo[dt.get_entity_id(\"Q5748\")]\n",
    "sevilla = modelo[dt.get_entity_id(\"Q8717\")]\n",
    "\n",
    "cervantes = modelo[dt.get_entity_id(\"Q5682\")]\n",
    "shakespeare = modelo[dt.get_entity_id(\"Q692\")]\n",
    "quijote = modelo[dt.get_entity_id(\"Q480\")]\n",
    "romeoyjulieta = modelo[dt.get_entity_id(\"Q83186\")]\n",
    "\n",
    "# Predecir la capital de Espa√±a\n",
    "vector = paris - francia + espanya # Funciona bastante bien\n",
    "\n",
    "# vector = paris - francia + catalunya # tambi√©n va bastante bien\n",
    "\n",
    "# Predecir obra de shakespeare\n",
    "# vector = quijote - cervantes + shakespeare # No funciona tan bien...\n",
    "# vector = romeoyjulieta - shakespeare + cervantes # Este tampoco...\n",
    "\n",
    "s = server.Server(si)\n",
    "simil = s.similarity_by_embedding(vector,10)\n",
    "for ent, dist in simil:\n",
    "    b_ent = dt.get_entity(ent)\n",
    "    print(\"[{0:7d}, {1:2.04f}] {2}\".format(ent, dist, b_ent))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
